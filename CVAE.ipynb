{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CVAE.ipynb","provenance":[],"collapsed_sections":["-o1zIJVm8SY7"],"authorship_tag":"ABX9TyOwyrMAbIvDZCQDcb8Kdv5P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Pmi1i6mGbKGi","executionInfo":{"status":"ok","timestamp":1633019312221,"user_tz":-120,"elapsed":8931,"user":{"displayName":"Fabian Wolf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09467462183684170769"}}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import time\n","\n","import tensorflow as tf\n","from tensorflow.keras import backend, optimizers, models, Model, applications, metrics, activations, losses\n","from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, LeakyReLU, Dropout, Flatten, Dense, Reshape, Concatenate, Conv2DTranspose, ReLU, Activation, Normalization, GlobalAveragePooling2D\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, TensorBoard\n","import tensorflow.keras.utils as utils"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"cdus7oyEbKwt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633019312223,"user_tz":-120,"elapsed":7,"user":{"displayName":"Fabian Wolf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09467462183684170769"}},"outputId":"131ea27d-a64d-444f-ddc4-04c76e541c5c"},"source":["from google.colab import drive\n","from google.colab import files \n","\n","\n","drive.mount(\"/content/gdrive\")\n","print(os.getcwd())\n","os.chdir(\"/content/gdrive/My Drive/AML2021/AML2021\")\n","print(os.getcwd())"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content\n","/content/gdrive/.shortcut-targets-by-id/1_nr6od538RmnZRE1gB6vQYFsrHaUgS19/AML2021/AML2021\n"]}]},{"cell_type":"markdown","metadata":{"id":"-o1zIJVm8SY7"},"source":["# Autoencoder"]},{"cell_type":"code","metadata":{"id":"b39VUoqfOdix","executionInfo":{"status":"ok","timestamp":1633018980770,"user_tz":-120,"elapsed":2,"user":{"displayName":"Fabian Wolf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09467462183684170769"}}},"source":["EPOCHS = 100\n","INITIAL_EPOCH = 0\n","BATCH_SIZE = 16\n","INPUT_SHAPE = (299,299,1)\n","CLASSES = 2\n","OPT = optimizers.Adam(learning_rate=1e-5)\n","VAL_SPLITT=0.2\n","os.makedirs('models/CVAE', exist_ok = True) \n","os.makedirs('models/CVAE/imgs', exist_ok = True)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_fzPH-r-8bnB"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"pgomxMkDT-wx"},"source":["# CVAE from https://www.tensorflow.org/tutorials/generative/cvae\n","class CVAE(tf.keras.Model):\n","  def __init__(self, encoder, decoder, name):\n","    super(CVAE, self).__init__(name=name)\n","    self.encoder = encoder# Erwartet als output 2*LATENT_SIZE einmal für mu einmal für sigma\n","    self.decoder = decoder #Erwartet als output logits ohne sigmoid(x), d.h. activation des letzten layers ist die identität: x->x\n","\n","  def encode(self, x):\n","    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n","    return mean, logvar\n","\n","  def reparameterize(self, mean, logvar):\n","    eps = tf.random.normal(shape=mean.shape)\n","    return eps * tf.exp(logvar * .5) + mean # transform standard normal to normal with mean, var\n","\n","  def decode(self, z):\n","    return self.decoder(z)\n","\n","  def forward(self, x):\n","    mean, logvar = self.encode(x)\n","    z = self.reparameterize(mean, logvar)\n","    x_logit = self.decode(z)\n","    return x, x_logit, z, mean, logvar\n","\n","# Encoder, Decoder from https://www.medrxiv.org/content/10.1101/2020.04.14.20065722v1.full.pdf\n","def blockT(i, filter, kernel, strides=(2,2), padding='same'):\n","  x = Conv2DTranspose(filter, kernel, strides=strides, padding=padding)(i)\n","  x = BatchNormalization()(x)\n","  x = ReLU()(x)\n","  return x\n","\n","def block(i, filter, kernel, strides=(2,2), padding='same'):\n","  x = Conv2D(filter, kernel, strides=strides, padding=padding)(i)\n","  x = BatchNormalization()(x)\n","  x = ReLU()(x)\n","  return x\n","\n","def create_encoder_decoder():\n","  x = Input(shape=INPUT_SHAPE)\n","  l = block(x, 64, (3,3), strides=(2,2), padding='same')\n","  l = block(l, 64, (3,3), strides=(1,1), padding='same')\n","  l = block(l, 128, (3,3), strides=(2,2), padding='same')\n","  l = block(l, 128, (3,3), strides=(1,1), padding='same')\n","  l = block(l, 256, (3,3), strides=(2,2), padding='same')\n","  l = block(l, 256, (3,3), strides=(1,1), padding='same')\n","  l = block(l, 512, (3,3), strides=(2,2), padding='same')\n","  l = Flatten()(l)\n","  o = Dense(2*81)(l) #LATENT_DIM is 9*9=81, das 2 mal steht davor weil mean und logvar predicted werden sollen und beide shape LATENT_DIM haben\n","  encoder = Model(x,o, name=\"encoder\")\n","\n","  z = Input(shape=(9*9,))\n","  l = Reshape((9,9,1))(z)\n","  l = blockT(l, 512, (3,3), strides=(2,2), padding='same')#18x18x512\n","  l = block(l, 512, (3,3), strides=(1,1), padding='same')\n","  l = blockT(l, 256, (3,3), strides=(2,2), padding='same')#36x36x256\n","  l = blockT(l, 128, (4,4), strides=(2,2), padding='valid')#74x74x128\n","  l = block(l, 128, (3,3), strides=(1,1), padding='same')\n","  l = blockT(l, 64, (3,3), strides=(2,2), padding='same')#148x148x64\n","  x = Conv2DTranspose(1, (5,5), strides=(2,2), padding='valid')(l)#299x299x1\n","  decoder = Model(z,x, name=\"decoder\")\n","  \n","  return encoder, decoder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zezi6fIKXDnn"},"source":["# Loss\n","def log_normal_pdf(sample, mean, logvar, raxis=1):\n","  log2pi = tf.math.log(2. * np.pi)\n","  return tf.reduce_sum(\n","      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n","      axis=raxis)\n","\n","def compute_loss(x, x_logit, z, mean, logvar):\n","  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n","  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n","  logpz = log_normal_pdf(z, 0., 0.)\n","  logqz_x = log_normal_pdf(z, mean, logvar)\n","  return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n","\n","def get_batch(batch_nr, indices):\n","  batch_start = batch_nr*BATCH_SIZE\n","  batch_end = batch_start+BATCH_SIZE\n","  batch_idx = indices[batch_start:batch_end]\n","  return x_train[batch_idx]\n","\n","# Fit\n","def fit(model, train_indices, val_indices):\n","  previous_val_loss = float('inf')\n","  for epoch in range(INITIAL_EPOCH, EPOCHS):\n","    train_losses = []\n","    val_losses = []\n","\n","    num_train_batches = int(np.ceil(train_indices.shape[0]/BATCH_SIZE))\n","    num_val_batches = int(np.ceil(val_indices.shape[0]/BATCH_SIZE))\n","    np.random.shuffle(train_indices)\n","    np.random.shuffle(val_indices)\n","    \n","    # training\n","    for batch_nr in range(num_train_batches):\n","      x = tf.cast(utils.normalize(get_batch(batch_nr, train_indices), axis=0), tf.float32)\n","      with tf.GradientTape() as tape:\n","        x, x_logit, z, mean, logvar = model.forward(x)\n","        loss = compute_loss(x, x_logit, z, mean, logvar)\n","      train_losses.append(loss.numpy())\n","      gradients = tape.gradient(loss, model.trainable_variables)\n","      OPT.apply_gradients(zip(gradients, model.trainable_variables))\n","      print(f\"\\r epoch {epoch}/{EPOCHS}, batch {batch_nr+1}/{num_train_batches}\", end=\"\")\n","    \n","    #validation\n","    for batch_nr in range(num_val_batches):\n","      x = tf.cast(utils.normalize(get_batch(batch_nr, train_indices), axis=0), tf.float32)\n","      x, x_logit, z, mean, logvar = model.forward(x)\n","      loss = compute_loss(x, x_logit, z, mean, logvar)\n","      val_losses.append(loss.numpy())\n","\n","    # reporting\n","    avg_train_loss = sum(train_losses) / len(train_losses)\n","    avg_val_loss = sum(val_losses) / len(val_losses)\n","    print(f\", train loss= {avg_train_loss}, val loss= {avg_val_loss}\" )\n","    with open(f\"models/CVAE/log-{model.name}.csv\",\"a+\") as f:\n","      f.write(f\"{epoch}, {avg_train_loss}, {avg_val_loss}\")\n","    \n","    # saving\n","    if avg_val_loss < previous_val_loss:\n","      model.encoder.save(f'models/CVAE/best-{model.name}-Encoder.hdf5')\n","      model.decoder.save(f'models/CVAE/best-{model.name}-Decoder.hdf5')\n","      print(f\"disc loss improved from {previous_val_loss} to {avg_val_loss}, best-{model.name}.hdf5 saved\")\n","    previous_val_loss = avg_val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vidDUzKI8eVF"},"source":["## Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"me7P6FXVaBbl","executionInfo":{"status":"ok","timestamp":1632422499420,"user_tz":-120,"elapsed":85000,"user":{"displayName":"Fabian Wolf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09467462183684170769"}},"outputId":"4603dbe5-d241-40b2-91af-84ece2126d33"},"source":["# Load Data\n","x_train = np.load(\"x_train.npy\")[:,:,:,0:1]# make rgb image to grayscale, this works because the original image was greyscale\n","y_train = np.load(\"y_train.npy\")\n","\n","x_cov_idx = np.where(y_train[:,0]==1)[0]\n","x_normal_idx = np.where(y_train[:,1]==1)[0]\n","\n","# train-val-split\n","np.random.shuffle(x_cov_idx)\n","train_cov_idx = x_cov_idx[int(x_cov_idx.shape[0]*VAL_SPLITT):]\n","val_cov_idx = x_cov_idx[:int(x_cov_idx.shape[0]*VAL_SPLITT)]\n","\n","np.random.shuffle(x_normal_idx)\n","train_normal_idx = x_normal_idx[int(x_normal_idx.shape[0]*VAL_SPLITT):]\n","val_normal_idx = x_normal_idx[:int(x_normal_idx.shape[0]*VAL_SPLITT)]\n","\n","print(x_train.shape)\n","print(train_cov_idx.shape, val_cov_idx.shape, train_normal_idx.shape, val_normal_idx.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(16324, 299, 299, 1)\n","(6530,) (1632,) (6530,) (1632,)\n"]}]},{"cell_type":"code","metadata":{"id":"NqU_RdDC4nOV"},"source":["# Train from scratch\n","INITIAL_EPOCH=0\n","enc_cov, dec_cov = create_encoder_decoder()\n","cvae_cov = CVAE(enc_cov, dec_cov, name=\"CVAE-Cov\")\n","fit(cvae_cov, train_cov_idx, val_cov_idx)\n","\n","INITIAL_EPOCH=0\n","enc_norm, dec_norm = create_encoder_decoder()\n","cvae_normal = CVAE(enc_norm, dec_norm, name=\"CVAE-Normal\")\n","fit(cvae_normal, train_normal_idx, val_normal_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_sUR1s88zr3J","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b3af462e-5ee1-4a3f-84b1-9e4525c8450f"},"source":["# Continue Training\n","INITIAL_EPOCH=100\n","EPOCHS = 130\n","enc_cov = models.load_model('models/CVAE/best-CVAE-Cov-Encoder.hdf5')\n","dec_cov = models.load_model('models/CVAE/best-CVAE-Cov-Decoder.hdf5')\n","cvae_cov = CVAE(enc_cov, dec_cov, name=\"CVAE-Cov\")\n","fit(cvae_cov, train_cov_idx, val_cov_idx)\n","\n","INITIAL_EPOCH=100\n","EPOCHS = 130\n","enc_norm = models.load_model('models/CVAE/best-CVAE-Normal-Encoder.hdf5')\n","dec_norm = models.load_model('models/CVAE/best-CVAE-Normal-Decoder.hdf5')\n","cvae_normal = CVAE(enc_norm, dec_norm, name=\"CVAE-Normal\")\n","fit(cvae_normal, train_normal_idx, val_normal_idx)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"," epoch 100/130, batch 205/205, train loss= 38643.77099847561, val loss= 38554.52489276961\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from inf to 38554.52489276961, best-CVAE-Cov.hdf5 saved\n"," epoch 101/130, batch 205/205, train loss= 38614.166577743905, val loss= 38471.56334252451\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38554.52489276961 to 38471.56334252451, best-CVAE-Cov.hdf5 saved\n"," epoch 102/130, batch 205/205, train loss= 38593.53212652439, val loss= 38535.59237132353\n"," epoch 103/130, batch 205/205, train loss= 38580.03077362805, val loss= 38561.81587009804\n"," epoch 104/130, batch 205/205, train loss= 38584.13809070122, val loss= 38507.88334865196\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38561.81587009804 to 38507.88334865196, best-CVAE-Cov.hdf5 saved\n"," epoch 105/130, batch 205/205, train loss= 38583.66337652439, val loss= 38440.830729166664\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38507.88334865196 to 38440.830729166664, best-CVAE-Cov.hdf5 saved\n"," epoch 106/130, batch 205/205, train loss= 38580.41945503049, val loss= 38509.591145833336\n"," epoch 107/130, batch 205/205, train loss= 38552.21303353659, val loss= 38539.004289215685\n"," epoch 108/130, batch 205/205, train loss= 38583.29550304878, val loss= 38540.34627757353\n"," epoch 109/130, batch 205/205, train loss= 38574.30386814025, val loss= 38406.725260416664\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38540.34627757353 to 38406.725260416664, best-CVAE-Cov.hdf5 saved\n"," epoch 110/130, batch 205/205, train loss= 38568.89016768293, val loss= 38575.870634191175\n"," epoch 111/130, batch 205/205, train loss= 38568.37250381098, val loss= 38508.598115808825\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38575.870634191175 to 38508.598115808825, best-CVAE-Cov.hdf5 saved\n"," epoch 112/130, batch 205/205, train loss= 38580.05146722561, val loss= 38415.56778492647\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38508.598115808825 to 38415.56778492647, best-CVAE-Cov.hdf5 saved\n"," epoch 113/130, batch 205/205, train loss= 38569.80712652439, val loss= 38474.119868259804\n"," epoch 114/130, batch 205/205, train loss= 38565.37425685976, val loss= 38522.41513480392\n"," epoch 115/130, batch 205/205, train loss= 38573.308250762195, val loss= 38452.207414215685\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38522.41513480392 to 38452.207414215685, best-CVAE-Cov.hdf5 saved\n"," epoch 116/130, batch 205/205, train loss= 38570.006726371954, val loss= 38501.93458946078\n"," epoch 117/130, batch 205/205, train loss= 38555.38069740854, val loss= 38542.47319240196\n"," epoch 118/130, batch 205/205, train loss= 38545.371493902436, val loss= 38433.40333946078\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38542.47319240196 to 38433.40333946078, best-CVAE-Cov.hdf5 saved\n"," epoch 119/130, batch 205/205, train loss= 38570.579134908534, val loss= 38535.78423713235\n"," epoch 120/130, batch 205/205, train loss= 38538.56101371951, val loss= 38458.14437806373\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38535.78423713235 to 38458.14437806373, best-CVAE-Cov.hdf5 saved\n"," epoch 121/130, batch 205/205, train loss= 38566.36732088414, val loss= 38462.419883578434\n"," epoch 122/130, batch 205/205, train loss= 38536.82136051829, val loss= 38478.13916973039\n"," epoch 123/130, batch 205/205, train loss= 38570.004058689025, val loss= 38329.497395833336\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38478.13916973039 to 38329.497395833336, best-CVAE-Cov.hdf5 saved\n"," epoch 124/130, batch 205/205, train loss= 38553.56282393292, val loss= 38514.59390318627\n"," epoch 125/130, batch 205/205, train loss= 38554.655602134146, val loss= 38624.71905637255\n"," epoch 126/130, batch 205/205, train loss= 38575.93065929878, val loss= 38562.05621936275\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38624.71905637255 to 38562.05621936275, best-CVAE-Cov.hdf5 saved\n"," epoch 127/130, batch 205/205, train loss= 38552.49702743902, val loss= 38578.80721507353\n"," epoch 128/130, batch 205/205, train loss= 38524.56114710366, val loss= 38511.35378370098\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 38578.80721507353 to 38511.35378370098, best-CVAE-Cov.hdf5 saved\n"," epoch 129/130, batch 205/205, train loss= 38509.27094131098, val loss= 38567.797257965685\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"," epoch 100/130, batch 205/205, train loss= 37741.52772484756, val loss= 37695.197150735294\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from inf to 37695.197150735294, best-CVAE-Normal.hdf5 saved\n"," epoch 101/130, batch 205/205, train loss= 37729.934375, val loss= 37727.036381740196\n"," epoch 102/130, batch 205/205, train loss= 37735.168997713416, val loss= 37688.59696691176\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37727.036381740196 to 37688.59696691176, best-CVAE-Normal.hdf5 saved\n"," epoch 103/130, batch 205/205, train loss= 37718.31726371951, val loss= 37599.09926470588\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37688.59696691176 to 37599.09926470588, best-CVAE-Normal.hdf5 saved\n"," epoch 104/130, batch 205/205, train loss= 37723.81669207317, val loss= 37629.556525735294\n"," epoch 105/130, batch 205/205, train loss= 37718.76737804878, val loss= 37696.482383578434\n"," epoch 106/130, batch 205/205, train loss= 37707.035194359756, val loss= 37686.42991727941\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37696.482383578434 to 37686.42991727941, best-CVAE-Normal.hdf5 saved\n"," epoch 107/130, batch 205/205, train loss= 37732.9423589939, val loss= 37702.146446078434\n"," epoch 108/130, batch 205/205, train loss= 37726.35594512195, val loss= 37649.78270526961\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37702.146446078434 to 37649.78270526961, best-CVAE-Normal.hdf5 saved\n"," epoch 109/130, batch 205/205, train loss= 37714.63608993903, val loss= 37642.38174019608\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37649.78270526961 to 37642.38174019608, best-CVAE-Normal.hdf5 saved\n"," epoch 110/130, batch 205/205, train loss= 37704.383460365854, val loss= 37579.13817401961\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37642.38174019608 to 37579.13817401961, best-CVAE-Normal.hdf5 saved\n"," epoch 111/130, batch 205/205, train loss= 37702.22496189024, val loss= 37688.93435968137\n"," epoch 112/130, batch 205/205, train loss= 37725.80897484756, val loss= 37628.556755514706\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37688.93435968137 to 37628.556755514706, best-CVAE-Normal.hdf5 saved\n"," epoch 113/130, batch 205/205, train loss= 37732.866787347564, val loss= 37683.95243566176\n"," epoch 114/130, batch 205/205, train loss= 37693.13530868902, val loss= 37648.68742340686\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37683.95243566176 to 37648.68742340686, best-CVAE-Normal.hdf5 saved\n"," epoch 115/130, batch 205/205, train loss= 37706.14052972561, val loss= 37599.96897977941\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37648.68742340686 to 37599.96897977941, best-CVAE-Normal.hdf5 saved\n"," epoch 116/130, batch 205/205, train loss= 37704.71932164634, val loss= 37731.099494485294\n"," epoch 117/130, batch 205/205, train loss= 37724.27902057927, val loss= 37667.423866421566\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37731.099494485294 to 37667.423866421566, best-CVAE-Normal.hdf5 saved\n"," epoch 118/130, batch 205/205, train loss= 37711.686356707316, val loss= 37590.68121936275\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37667.423866421566 to 37590.68121936275, best-CVAE-Normal.hdf5 saved\n"," epoch 119/130, batch 205/205, train loss= 37711.50506859756, val loss= 37600.32391237745\n"," epoch 120/130, batch 205/205, train loss= 37704.90916539634, val loss= 37716.62997855392\n"," epoch 121/130, batch 205/205, train loss= 37703.938833841465, val loss= 37706.54710477941\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37716.62997855392 to 37706.54710477941, best-CVAE-Normal.hdf5 saved\n"," epoch 122/130, batch 205/205, train loss= 37702.879439786586, val loss= 37733.974417892154\n"," epoch 123/130, batch 205/205, train loss= 37708.21863567073, val loss= 37708.03760723039\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37733.974417892154 to 37708.03760723039, best-CVAE-Normal.hdf5 saved\n"," epoch 124/130, batch 205/205, train loss= 37698.065205792685, val loss= 37618.462928921566\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","disc loss improved from 37708.03760723039 to 37618.462928921566, best-CVAE-Normal.hdf5 saved\n"," epoch 125/130, batch 205/205, train loss= 37681.445655487805, val loss= 37658.517769607846\n"," epoch 126/130, batch 205/205, train loss= 37692.37635289634, val loss= 37664.88740808824\n"," epoch 127/130, batch 165/205"]}]},{"cell_type":"markdown","metadata":{"id":"XJyoWwQ38WcR"},"source":["# Classifier"]},{"cell_type":"code","metadata":{"id":"1RpMFRGh6792","executionInfo":{"status":"ok","timestamp":1633019511771,"user_tz":-120,"elapsed":584,"user":{"displayName":"Fabian Wolf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09467462183684170769"}}},"source":["INITIAL_EPOCH = 0\n","EPOCHS = 120\n","BATCH_SIZE = 16\n","INPUT_SHAPE = (299,299,1)\n","CLASSES = 2\n","VAL_SPLITT = 0.2\n","OPTIMIZER = optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=False)\n","\n","METRICS = [metrics.AUC(name='auroc', curve='ROC'),\n","           metrics.CategoricalAccuracy(name='accuracy'),\n","           metrics.Precision(name='precision'),\n","           metrics.Recall(name='recall')\n","           ]\n","\n","LRS = ReduceLROnPlateau(monitor='val_auroc', factor=0.1, patience=5, verbose=1, mode='auto', min_delta=0.0001)\n","ES = EarlyStopping(monitor='val_auroc', min_delta=0.0001, patience=9, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n","\n","\n","MC_BEST = ModelCheckpoint(f'models/CVAE/best-CLS.hdf5', monitor='val_auroc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq=\"epoch\")\n","LOG = CSVLogger(f'models/CVAE/CLS.log')\n","TB = TensorBoard(log_dir=f'models/CVAE/logs')"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"2b7GckE9hJGX","executionInfo":{"status":"ok","timestamp":1633019453068,"user_tz":-120,"elapsed":132981,"user":{"displayName":"Fabian Wolf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09467462183684170769"}}},"source":["# Load Data\n","x_train = np.load(\"x_train.npy\")[:,:,:,0:1]# make rgb image to grayscale, this works because the original image was greyscale\n","x_normalized = np.zeros(x_train.shape, dtype=\"float32\")\n","idx = np.arange(0,x_train.shape[0])\n","num_batches = int(np.ceil(idx.shape[0]/BATCH_SIZE))\n","for batch_nr in range(num_batches):\n","  batch_start = batch_nr*BATCH_SIZE\n","  batch_end = batch_start+BATCH_SIZE\n","  batch_idx = idx[batch_start:batch_end]\n","  x_normalized[batch_idx] = tf.cast(utils.normalize(x_train[batch_idx], axis=0), tf.float32)\n","x_train = []\n","\n","y_train = np.load(\"y_train.npy\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"JkMFvdHNfM8J","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b2beac4e-6de5-4a68-95c9-abef75ca0ddc"},"source":["e_cov = models.load_model(\"models/CVAE/best-CVAE-Cov-Encoder.hdf5\")\n","d_cov = models.load_model(\"models/CVAE/best-CVAE-Cov-Decoder.hdf5\")\n","e_norm = models.load_model(\"models/CVAE/best-CVAE-Normal-Encoder.hdf5\")\n","d_norm = models.load_model(\"models/CVAE/best-CVAE-Normal-Decoder.hdf5\")\n","\n","e_cov.trainable = False\n","d_cov.trainable = False\n","e_norm.trainable = False\n","d_norm.trainable = False\n","\n","e_cov._name = \"e_cov\"\n","d_cov._name = \"d_cov\"\n","e_norm._name = \"e_norm\"\n","d_norm._name = \"d_norm\"\n","\n","\n","\n","x = Input(shape=INPUT_SHAPE, batch_size=BATCH_SIZE)\n","\n","mean_cov, _ = tf.split(e_cov(x), num_or_size_splits=2, axis=1)\n","x_recon_cov = d_cov(mean_cov)\n","\n","mean_norm, _ = tf.split(e_norm(x), num_or_size_splits=2, axis=1)\n","x_recon_normal = d_norm(mean_norm)\n","\n","x_0 = tf.abs(x-x_recon_cov)\n","x_1 = tf.abs(x-x_recon_normal)\n","x_2 = tf.abs(2*x-x_recon_normal)\n","l = tf.concat([x_0,x_1,x_2], axis=-1)# now shape should be None, 299, 299, 3\n","\n","base_model = applications.ResNet50(include_top=False, weights=\"imagenet\", input_shape=(299,299,3))\n","l = base_model(l)\n","l = GlobalAveragePooling2D(name='avg_pool')(l)\n","y = Dense(CLASSES, activation='softmax', name=f'fc{CLASSES}')(l)\n","\n","model = Model(x,y)\n","model.compile(OPTIMIZER,  loss='categorical_crossentropy', metrics=METRICS)\n","\n","model.fit(x=x_normalized, y=y_train, batch_size=BATCH_SIZE, validation_split = VAL_SPLITT, callbacks = [MC_BEST, LRS, ES, LOG, TB], epochs = EPOCHS, initial_epoch = INITIAL_EPOCH)\n","x_train=[]\n","y_train=[]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/120\n"]}]},{"cell_type":"code","metadata":{"id":"Sw4JAZGFsRE4"},"source":["# Eval Model\n","x_test = tf.cast(utils.normalize(np.load(\"x_test.npy\")[:,:,:,0:1], axis=0), tf.float32)\n","y_test = np.load(\"y_test.npy\")\n","\n","model = models.load_model(f'models/CVAE/best-CLS.hdf5', compile=True)\n","out = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=1)\n","\n","# Save Results\n","with open(\"results.txt\",\"a+\") as f:\n","  f.write(f\"CVAE-CLS test_loss: {out[0]}, test_auroc: {out[1]}, test_acc: {out[2]}, test_prec: {out[3]}, test_rec: {out[4]}, test_f1: {2* ((out[3]*out[4]) / (out[3]+out[4]))}\\n\")\n","print(f\"wrote \\\"best-CLS test_loss: {out[0]}, test_auroc: {out[1]}, test_acc: {out[2]}, test_prec: {out[3]}, test_rec: {out[4]}, test_f1: {2* ((out[3]*out[4]) / (out[3]+out[4]))}\\n\\\" to results.txt \")"],"execution_count":null,"outputs":[]}]}